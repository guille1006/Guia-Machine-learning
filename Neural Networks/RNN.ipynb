{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1f9e11",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 3em; font-weight: bold; text-align: center;\">Recurrent Neural Networks</div>\n",
    "<div style=\"text-align: right; font-size: 24px; margin-right: 10px;\">Guillermo Díaz Aguado</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de719127",
   "metadata": {},
   "source": [
    "# Introducción.\n",
    "Las demás arquitecturas neuronales trabajadas son diseñadas para datos multidimensionales donde <u>no hay ningún orden</u> y <u>el número de dimensiones está fijo</u>.\n",
    "Pero hay otros campos donde **el orden** es importante como:\n",
    "* *Time series*. Si cambiamos el orden temporal de los valores, se pierde el hilo. Un punto importante es que las observaciones son parecidas a la observacion anterior.\n",
    "* *Texto*. Debe seguir un orden para que la frase tenga sentido y el sentido que queremos darle. \n",
    "* *Biological data*. ADN...\n",
    "\n",
    "Las **Redes Neuronales Recurrentes** son una familia especializada en **procesar datos secuenciales**. \n",
    "\n",
    "Para entender este tipo de redes neuronales debemos tener en cuenta una nueva idea: **Compartir parametros entre diferentes partes del modelo**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd716b3",
   "metadata": {},
   "source": [
    "## Definiciones iniciales:\n",
    "* Secuencia de datos: $\\mathbf{x}^{t}$\n",
    "    * Time step: $t$\n",
    "    * Secuencia que va desde 1 a $\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc52803",
   "metadata": {},
   "source": [
    "# 1. Computational Graphs. \n",
    "## 1.1. Unfolding Computational Graphs.\n",
    "Un **gráfico computacional** es la forma de formalizar la estructura de un conjunto de computaciones. La idea de **desenvolver** una *computación recursiva o recurrente* en un gráfico computacional que tiene una estructura repetitiva, típicamente correspone a una *cadena de eventos*.\n",
    "\n",
    "Pongamos como ejemplo intuitivo de una función dinámica tradicional para entender la estructura de las RNN:\n",
    "$$\n",
    "s^{(t)}=f(s^{(t-1)}, x^{(t)};\\theta)\n",
    "$$\n",
    "Donde $x{(t)}$ es una señal externa.\n",
    "\n",
    "Si queremos saber como sería la función para un $\\tau =3$ la ecuación se quedaría:\n",
    "$$\n",
    "s^{(3)}=f(s^{(2)}, x^{(2)};\\theta)=f(f(s^{(1)}, x^{(1)};\\theta), x^{(2)};\\theta)\n",
    "$$\n",
    "\n",
    "Podemos dibujarlo como un gráfico, donde mi preferido es el izquierdo: \n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_RNN/Grafico_RNN.png\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee061f",
   "metadata": {},
   "source": [
    "## 1.2. Recurrent Neural Network Computational Graph.\n",
    "Algunos de los diseños de patrones más importantes para redes neuronales recurrentes son los siguientes:\n",
    "\n",
    "\n",
    "### 1.2.1 RNN Many-to-Many\n",
    "Redes recurrentes que producen una salida a cada paso y tiene conexiones recurrentes entre las neuronas ocultas. \n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_RNN/RNN_Many_to_many.png\">\n",
    "</div>\n",
    "\n",
    "* $\\mathbf(y)$: Target/Objetivo\n",
    "* $\\mathbf(L)$: Función de perdida\n",
    "* $\\mathbf(o)$: Capa de Salida\n",
    "    * $\\mathbf(V)$: Matriz de pesos de capa de salida\n",
    "* $\\mathbf(h)$: Capas ocultas\n",
    "    * $\\mathbf(W)$: Matriz de pesos de capas ocultas **con conexión recurrente**\n",
    "* $\\mathbf(x)$: Capa de Entrada\n",
    "    * $\\mathbf(U)$: Matriz de pesos de entrada. Suele ser matriz Unitaria\n",
    "\n",
    "En esta estructura, podemos ver que <u>conexiones recurrentes entre las conexiones ocultas</u> las cuales están parametrizadas por una matriz de pesos $\\mathbf(W)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83012ae8",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.2 Output Feedback RNNs. Redes con retroalimentación de salida \n",
    "Redes recurrentes que producen una salida a cada paso y tiene conexiones recurrentes que proviene solo de la señal de salida y va hacía las neuronas ocultas.\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_RNN/RNN_Output_feedback.png\">\n",
    "</div>\n",
    "\n",
    "* $\\mathbf(y)$: Target/Objetivo\n",
    "* $\\mathbf(L)$: Función de perdida\n",
    "* $\\mathbf(o)$: Capa de Salida\n",
    "    * $\\mathbf(V)$: Matriz de pesos de capa de salida\n",
    "* $\\mathbf(h)$: Capas ocultas\n",
    "    * $\\mathbf(W)$: Matriz de pesos de capas ocultas **con conexión recurrente a la capa de salida anterior**\n",
    "* $\\mathbf(x)$: Capa de Entrada\n",
    "    * $\\mathbf(U)$: Matriz de pesos de entrada. Suele ser matriz Unitaria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a9af5",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.3 RNN Many-to-One\n",
    "Redes recurrentes con conexiones recurrentes entre neuronas oculatas que leen la secuencia entera y devuleven una única salida.\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_RNN/RNN_Many_to_One.png\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
