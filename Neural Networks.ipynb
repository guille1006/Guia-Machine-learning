{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Neural Networks</h1>\n",
    "<h1 style=\"text-align: right; font-size: 24px; margin-right: 10px;\">Guillermo Díaz Aguado</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción \n",
    "La idea central es extraer combinaciones lineales de las entradas como features combinadas, y luego modelizar el target como una función no lineal de esas features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection Pursuit Regression \n",
    "Como todos los problemas de aprendizaje supervisado, tenemos un vector de entrada X con *p* variables independientes y un target Y. Usaremos unos pesos $\\omega_m$ siendo $m=1,2,...,p$. El *Prjection Pursuit Regression* PPR tendrá la forma:\n",
    "$$ \n",
    "f(X)=\\sum_{m=1}^{p}g_m(\\omega_{m}^TX)\n",
    "$$\n",
    "Donde muchas veces para simplificar usaremos:\n",
    "$$\n",
    "V_m = \\omega_{m}^TX\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Actualmente el nombre de red neural ha derivado en una gran cantidad de modelos y metodos de aprendizaje. Por ahora vamos a describir el modelo mas sencillo y basico (el modelo mas \"vanilla\") llamado tambien como el Perceptron de una capa oculta o the single layer back-propagation network. Usualmente son vistos como una especie de magia que proviene de los ordenadores, pero realmente se puede ver como modelos estadíticos no lineales. \n",
    "\n",
    "Son usados para problemas de clasificación y de regresión, en el caso de tener un problema de regresión nuestra capa fianl estará compuesta por un único nodo que nos indicará el resultado, en cambio si es un problema de clasificación con *k* posibles valores nuestra capa final estará compuesta por *k* nodos, donde cada uno tendrá su posibilidad de ser. \n",
    "\n",
    "Empezaremos con los nodos de nuestras variables independientes: $X^n$, todos estos nodos de inicio pasarán sus valores a cada uno de los nodos de la siguiente capa de nuestra red, dentro de cada nodo interno se calculará el valor de estos nodos internos como una función lineal de la capa anterior, nosotros lo llamaremos $Z_m$ donde *m* será el número de nodos de la capa \n",
    "$$ Z_m = \\sigma(\\alpha_{0m}+ \\alpha_{m}^{T}X)$$\n",
    "* La **función de activación $\\sigma(v)$** será aquella que usaremos para hacer que los valores de cada nodo varien entre 0 y 1. \n",
    "* Realmente cada nodo asocia a los nodos anteriores un *bias*, pero como al final es una función lineal de la capa anterior, podemos juntar los *bias* como un único valor. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
