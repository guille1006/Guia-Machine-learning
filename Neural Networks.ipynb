{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Neural Networks</h1>\n",
    "<h1 style=\"text-align: right; font-size: 24px; margin-right: 10px;\">Guillermo Díaz Aguado</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción \n",
    "La idea central es extraer combinaciones lineales de las entradas como **features derivadas**, y luego modelizar el target como una función no lineal de esas features. \n",
    "\n",
    "### Definiciones iniciales:\n",
    "* Neuron/Nodo: Elemento que alamcena un número. Generalmente un valor entre 0 y 1\n",
    "* Layer/Capa: Es un conjunto de neuronas que aplican una operación matemática específica (como una transformación lineal seguida de una función de activación) sobre su entrada, y cuyo resultado se transmite a la siguiente capa.\n",
    "    * Input Layer/ Capa de Entrada: Son los datos de las variables independientes\n",
    "    * Hidden Layer / Capas Ocultas: Son las capas intermedias donde se realizan los cálculos para entender el modelo.\n",
    "        * La **profundidad** indica el número de capas ocultas que tiene el modelo.\n",
    "        * Los cálculos se realizan con: \n",
    "            * Los **weights/pesos**: $\\alpha$\n",
    "            * Los **bias/sesgos**: $\\alpha_0$\n",
    "            * La **función de activación**: $\\sigma$\n",
    "            * ***p***: número de componentes de *X*, su rango es 1,..., p\n",
    "            * ***K***: número de valores de salida, su rango es -> k= 1,...,K\n",
    "            * ***M***: número de nodos en una capa, su rango es -> m= 1,...,M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Pursuit Regression \n",
    "Como todos los problemas de aprendizaje supervisado, tenemos un vector de entrada X con *p* variables independientes y un target *y*. Usaremos unos pesos $\\omega_m$ siendo $m=1,2,...,p$. El *Prjection Pursuit Regression* PPR tendrá la forma:\n",
    "$$ \n",
    "f(X)=\\sum_{m=1}^{p}g_m(\\omega_{m}^TX)\n",
    "$$\n",
    "Donde muchas veces para simplificar usaremos:\n",
    "$$\n",
    "V_m = \\omega_{m}^TX\n",
    "$$\n",
    "\n",
    "La función $g_m(\\omega_{m}^TX)$ es llamada la *ridge function* in el espacio $\\mathbb{R}^p$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Actualmente el nombre de red neural ha derivado en una gran cantidad de modelos y metodos de aprendizaje. Por ahora vamos a describir el modelo mas sencillo y basico (el modelo mas \"vanilla\") llamado tambien como el Perceptron de una capa oculta o the single layer back-propagation network. Usualmente son vistos como una especie de magia que proviene de los ordenadores, pero realmente se puede ver como modelos estadíticos no lineales. \n",
    "\n",
    "Son usados para problemas de clasificación y de regresión, en el caso de tener un problema de regresión nuestra capa fianl estará compuesta por un único nodo que nos indicará el resultado, en cambio si es un problema de clasificación con *k* posibles valores nuestra capa final estará compuesta por *k* nodos, donde cada uno tendrá su posibilidad de ser. \n",
    "\n",
    "Empezaremos con los nodos de nuestras variables independientes: $X^n$, todos estos nodos de inicio pasarán sus valores a cada uno de los nodos de la siguiente capa de nuestra red, dentro de cada nodo interno se calculará el valor de estos nodos internos como una función lineal de la capa anterior, nosotros lo llamaremos $Z_m$ donde *m* será el número de nodos de la capa \n",
    "$$ \n",
    "\\begin{align*}\n",
    "Z_m &= \\sigma(\\alpha_{0m}+ \\alpha_{m}^{T}X) && Z_m: \\text{Variable derivada} \\\\\n",
    "&                                          && \\sigma: \\text{Función de activación} \\\\\n",
    "&                                          && \\alpha_{0m}: \\text{bias} \\\\ \n",
    "&                                          && \\alpha_{m}^{T}: \\text{Peso para el nodo m de la capa T} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "* La **función de activación $\\sigma(v)$** será aquella que usaremos para hacer que los valores de cada nodo varien entre 0 y 1. \n",
    "* Realmente cada nodo asocia a los nodos anteriores un *bias*, pero como al final es una función lineal de la capa anterior, podemos juntar los *bias* como un único valor. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las unidades en la parte central de la red son llamadas *hidden unitis*, porque los valores de $Z_m$ no son observados directamente. Podemos imaginar los valores de $Z_m$ como una *expansión de las bases* de las entradas originales X; la **red neuronal** es entonces un modelo lineal estándard, o un modelo multilogistico lineal (usando estas transformaciones como entradas). Pero hay una mejora: en este caso los parametros de la función base son <u> aprendidos desde los datos</u>.\n",
    "Es por ello que, si la función de activación $\\sigma$ fuese la *función identidad*, entonces el modelo complpeto colapsaría en un modelo lineal.\n",
    "\n",
    "De tal forma, una red neuronal se puede describir como una generalización **no-lineal** de un modelo lineal mediante la introducción de la función de activación $\\alpha$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Neural Networks\n",
    "Las redes neuronales en un principio tienen parametros desconocidos: los **weights(pesos)**, es nuestro trabajo encontrar valores para estos pesos, y conseguir que el modelo encaje bien con los datos de entrenamiento. \n",
    "Tendremos nuestro set completo de pesos de la siguiente manera:\n",
    "$$\n",
    "\\begin{align*}\n",
    "{\\alpha_{0m}, \\alpha_m; m=1,2,\\dots, M}&\\quad M(p+1) weights \\\\\n",
    "{\\beta_{0k}, \\beta_k; k=1,2,\\dots,K} &\\quad K(M+1) weights\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Para regresión, usaremos el SSE *sum-of-squared errors* como nuestra medida de encaje (**función de error**)\n",
    "$$\n",
    "R(\\theta)=\\sum_{k=1}^K\\sum_{i=1}^N(y_{ik}-f_k(x_i))^2\n",
    "$$\n",
    "\n",
    "* Para clasificación podemos usar *squared error* o *cross-entropy* como **función de error**:\n",
    "$$\n",
    "R(\\theta)=\\sum_{k=1}^K\\sum_{i=1}^Ny_{ik}\\text{log}f_k(x_i)\n",
    "$$\n",
    "\n",
    "De forma general no queremos el **minimo global de $R(\\theta)$**, ya que es muy probable que nos de una solución <u>sobreajustada</u>. Para que no se dé el sobreajuste podremos hacerlo de varias formas:\n",
    "* De manera directa: Con un **término de penalización**\n",
    "* De manera indirecta: Con un **early stopping**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algunos problemas entrenando Redes Neuronales.\n",
    "Entrenar una red neuronal es casi un arte. Los modelos generalmente están sobreparametrizados, y el problema de la optimización es *no-convexo* e *inestable*, por ello algunas reglas deben aplicarse. Son las siguientes reglas:\n",
    "\n",
    "## Valores iniciales. \n",
    "Supongamos que iniciamos los pesos (y todos los parametros del modelo) con un valor igual a 0. Debido a como se realiza el **gradient descent**, muchas veces podemos toparnos con zonas concavas que dan a un mínimo local, el cuál puede estar lejos de la solución. Al ser un mínimo, es gradient descent no nos da ninguna dirección de adonde seguir, quedandonos estancados en una cuenca con un mínimo local.\n",
    "La mejor forma es realizar varios cálculos del modelo, donde los <u> valores inicales de los parametros del modelo son elegidos aleatoriamente</u>\n",
    "Uno deberá probar con unos cuantos *puntos iniciales aleatorios*, y elegir aquel que dé la solución menos penalizada. Aunque una solción mas acertada es elegir <u>la media de las predicciones</u> de cada punto inicial. La solución anterior es mejor que hacer la media de los pesos, ya que la *no-lienealidad* del modelo implica que este procedimiento pueda ser pobre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting (sobreajuste)\n",
    "Usualemente las redes neuronales tienen demasiados pesos lo que provoca que el modelo se *sobreajuste* al mínimo global de $R$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalado de las entradas\n",
    "Tal y como pasa en los modelos linales, es necesario aplicar un escalado a las variables para que aquellas de mayor magnitud no opaquen aquellas de menor magnitud. Al principio, es mejor estandarizar todas las entradas para que tengan media=0 y std desviation=1. \n",
    "Además de estadarizar las magnitudes, también nos permite elegir entre un rango uniforme de pesos aleatorios de entrada [-0.7, +0.7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número de nodos y capas escondidas (hidden units and layers)\n",
    "Generalmente hablando, es mejor tener **muchos nodos escondidos** que **muy pocos nodos escondidos**. \n",
    "* Con <u>muy pocos nodos</u> el modelo tal vez no tenga la suficiente elasticidad para capturar las *no-linealidades*\n",
    "* Con <u>muchos nodos</u> los *nodos extra* tendrán valor casi **nulo** si la regularización es buena. \n",
    "Para elegir las capas internas se necesita un conocimiento del dataset y experimentación. Cada capa extrae caracteristicas de las entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZIP Code Data\\ MNIST (Modified National Institute of Standards and Technology\n",
    "Este ejemplo es una tarea de reconocimento de números escritos a mano.\n",
    "Se presenta como datos de entrada: los valores de cada pixel de un cuadro 28x28 que representa un número escrito a mano. Los valores de cada pixel va de:\n",
    "* 0: Completamente negro \n",
    "* 1: Completamente blanco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargamos el dataset de MNIST\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizamos los datos de entrada:\n",
    "X_train, X_test = X_train/255.0, X_test/255.0\n",
    "\n",
    "plt.imshow(X_train[9])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
