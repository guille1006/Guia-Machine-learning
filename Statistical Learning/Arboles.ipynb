{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcace9c",
   "metadata": {},
   "source": [
    "# Árboles de decisión\n",
    "\n",
    "Un árbol de clasificación/regresión es el resultado de preguntar una secuencia ordenada de cuestiones. Las cuestiones que se plantean en cada etapa dependen de las respuestas a las cuestiones previas de la secuencia\n",
    "\n",
    "Las cuestiones se ordenan de más importante a menos importante.\n",
    "\n",
    "Los árboles más usados son los dedecisión binarios, en los que cada nodo padre se divide en **dos nodos descendiente**. La división binaria queda determinada por una condición booleana.\n",
    " \n",
    "\n",
    "| **VENTAJAS** | **DESVENTAJAS**|\n",
    "|---|---|\n",
    "| Gran potencia descriptiva, se comprende muy bien el resultado  | Poca fiabilidad y mala generalización: cada hoja es un parámetro y esto provoca modelos sobreajustados e inestables para la predicción. |\n",
    "| Descubren interacciones y reglas difíciles de encontrar en otros métodos (útil como variables dummy para otros modelos).                 | Añadir una variable nueva o un nuevo conjunto de observaciones puede influir mucho en el árbol. |\n",
    "| Las relaciones no lineales no afectan tanto al comportamiento de los árboles.                                                            | Poca eficacia predictiva, sobre todo en regresión: toscos en los valores de predicción.                                                 |\n",
    "| Manera propia y eficiente de tratar los missings incorporada al proceso                                                                  | Raramente son el modelo final, se suelen usar como apoyo a otros modelos.                                                               |\n",
    "\n",
    "\n",
    "Debido a su estructura y planteamiento, este algoritmo es especialmente indicado cuando la **explicabilidad de un modelo es fundamental**\n",
    "\n",
    "Pero **no son adecuados** para algunos casos donde se presentarían MUCHAS ramas, resultando en un árbol muy complejo y difícil de interpretar. cuando:\n",
    "\n",
    "* Existe una gran cantidad de variables nominales con muchos niveles\n",
    "\n",
    "* Hay una gran cantidad de variables numericas\n",
    "\n",
    "Usa el método *divide y vencerás*\n",
    "\n",
    "Los arboles se comportan como analísi de cluster (creación de grupos homogéneos y diferentes entre si) y a la vez como un **modelo predictivo**\n",
    "\n",
    "## Creación del árbol de decisión.\n",
    "\n",
    "<ol>\n",
    "  <li>Usa una división binaria para empezar creando un gran árbol con los datos de entrenamiento, solo se para de seguir creando ramas cuando una hoja ha llegado al <span style=\"color:indianred\">minimun number of observations</span></li>\n",
    "  <li>Aplicamos un <span style=\"color:lightblue\">PODADO</span> del árbol creado, como función de <span style=\"color:indianred\">&alpha;</span></li>\n",
    "  <li>Usamos K-fold cross-validation para elegir <span style=\"color:indianred\">&alpha;</span>.</li>\n",
    "  <li> Devolvemos el subarbol con <span style=\"color:indianred\">&alpha;</span> que corresponde al CV realizado</li>\n",
    "</ol>\n",
    "\n",
    "## Como crear el árbol de decisión.\n",
    "\n",
    "En cada nodo, el algoritmo de generación del árbol tiene que decidir sobre que variables es “optima” la partición. Hay que considerar cada posible divisón sobre todas las variables presentes y decidir cual es la mejor según algún criterio.\n",
    "\n",
    "### Crtiterio para la división de nodos.\n",
    "\n",
    "El objetivo es encontrar en los datos el punto de corte que divida estos en dos grupos lo mas “puros” posibles. Esto consiste en asegurar que un nodo padre produzca nodos hijos donde la división de clases sea lo “mas pura” posible. El caso más optimo es donde podemos dividir claramente en dos clases los datos.\n",
    "\n",
    "La pureza se puede medir con\n",
    "\n",
    "* Entropia\n",
    "\n",
    "* Indice de Gini\n",
    "\n",
    "* Minima probabilidad\n",
    "\n",
    "### Entropia\n",
    "\n",
    "La entropía es una medida de la incertidumbre o aleatoriedad de un conjunto de datos. En un nodo, se calcula la entropía de la distribución de las clases objetivo(etiquetas de los datos) presentes en ese nodo. Un valor de entropía alto indica mayor mezcla o *impureza*\n",
    "\n",
    "### Indice de Gini\n",
    "\n",
    "El inidice de Gini es una medida de desigualdad, la cual ofrece un valor de homogeneidad/heterogeneidad de los datos. Los valores minimos indican pureza. Valores altos, son datos distintos entre si\n",
    "\n",
    "### Minima Probabilidad.\n",
    "\n",
    "Fijar un umbral de probabilidad mínima para considerar una partición en el árbol. Es útil cuando se quiere reducir la impureza en base a la proporción de clases y no a su “mezcla”. Por ejemplo, cunado hay clases desequilibradas y se quiere garantizar que las divisiones se realicen solo cuanod la probabilidad de una clase en particular sea superior a un valor mínimo.\n",
    "\n",
    "### Nodos terminales.\n",
    "\n",
    "El proceso de segmentación recursiva continua hasta que el árbol se **satura**, en el sentido que los sujetos en los **nodos descendientes no se pueden partir mas**. Cualquier nodo que no so pueda o no sea dividido es un *nodo terminal*\n",
    "\n",
    "### Como se eligen los puntos de corte\n",
    "\n",
    "* **VARIABLE CUALITATIVA** se eligen en función de si pertenece a un grupo especifico de una variable o no.\n",
    "\n",
    "* **VARIABLE CONTINUA** se van probando distintos cortes, en función de si es mayor omenor a un valor.\n",
    "\n",
    "Los arboles actúan en *cascada* tomando **una decisión para cada variable**\n",
    "\n",
    " \n",
    "\n",
    "## Medidas y proceso para la construcción de un árbol.\n",
    "\n",
    "El procesi de creación de árbol es laborioso por la *complicada casuistica y combinatoria* que se va generando cada vez que es necesario dividir un nodo. Algunos algoritmos creados para que vaya algo mejor son:\n",
    "\n",
    "### CART (Classification and Regression Trees)\n",
    "\n",
    "Consiste en construer **Un ARBOL GRANDE** y acto seguido **IR PODANDO** hasta dar con el tamaño correcto.\n",
    "\n",
    "1. Construir el árbol saturado (hasta que no se puede hacer mas divisiones)\n",
    "\n",
    "2. Elección del tamaño correcto\n",
    "\n",
    "3. Predicción\n",
    "\n",
    "### CHAID\n",
    "\n",
    "Examina las tablas cruzadas entre los campos de entrada y los resultados para, a continuación, comprobar la significación mediante una comprobación de independencia de chi-cuadrado.\n",
    "\n",
    "## Arbol final, subárboles y prunnnig\n",
    "\n",
    "El algoritmo finaliza cuando se cumple alguno de los criterios de parada.\n",
    "\n",
    "* No hay suficientes observaciones en las hojas finales para considerar una división\n",
    "\n",
    "* Se alcanza la profundidad máxima\n",
    "\n",
    "* En ningún nodo se puede ,ejorar el criterio de división\n",
    "\n",
    "## Poda o Prunning\n",
    "\n",
    "Un árbol puede crecer casi indefinidamente en particiones mas pequeñas hasta que cada observación sea una rama. Estas soluciones tan especificas darían *overfitting*\n",
    "\n",
    "El **proceso de poda** asegura la generalización\n",
    "\n",
    "* Pre-poda: determinar un numero especifico de decisiones o establecer un mínimo de casos por nodo.\n",
    "\n",
    "* Post-poda: Permite el proceso de crecimiento para que a posteriori se corten las ramas\n",
    "\n",
    "### Prunning en CART\n",
    "\n",
    "Se usa el parámetro de control de complejidad *cp*, que establece “penalizaciones” se se producen muchas divisiones. El valor predeterminado es *cp*=0.01\n",
    "\n",
    "* Cuanto mas alto cp, mas pequeño será el árbol\n",
    "\n",
    "Un valor demasiado pequeño provoca *overfitting*, y un valor demasiado grande nos da un árbol pequeño.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "# Bagging\n",
    "\n",
    "Es un proceso de ensamblado se podría usar en cualquier tipo de modelos, pero se ha demostrado que solo es útil con árboles.\n",
    "Es importatne tener ajustado el numero mínimo de observaciones en una hoja para ahorrarse pruebas en bagging.\n",
    "La idea del submuestreo es controlar el sobreajuste implicito en la selección rigida de variables y estimación fija de parámetros\n",
    "En general el bagging funciona bien cuando:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wokdqpdkpaekda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
