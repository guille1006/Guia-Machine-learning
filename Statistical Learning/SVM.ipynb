{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ce45b6",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 3em; font-weight: bold; text-align: center;\">Support Vector Machine</div>\n",
    "<div style=\"text-align: right; font-size: 24px; margin-right: 10px;\">Guillermo Díaz Aguado</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28add09b",
   "metadata": {},
   "source": [
    "# Definición Máquinas de Vector Soporte\n",
    "## Hiperplanos\n",
    "En un espacio p-dimensional, un **hiperplano** es un subespacio plano afín de p-1 dimensiones. La ecuación para estos planos sería algo tal que así:\n",
    "\n",
    "$$\n",
    "\\beta_0X_0+\\beta_1X_1+...+\\beta_pX_p=0\n",
    "$$\n",
    "\n",
    "Donde todos los puntos donde se verifique la ecuación anterior estarán incuidos en el hiperplano. El hiperplano divide el espacio 3 partes:\n",
    "* Aquellos puntos incluidos en el hiperplano:\n",
    "$$\n",
    "\\beta_0X_0+\\beta_1X_1+...+\\beta_pX_p=0\n",
    "$$\n",
    "* Aquellos puntos por encima del hiperplano:\n",
    "$$\n",
    "\\beta_0X_0+\\beta_1X_1+...+\\beta_pX_p>0\n",
    "$$\n",
    "* Los puntos que están por debajo del hiperplano:\n",
    "$$\n",
    "\\beta_0X_0+\\beta_1X_1+...+\\beta_pX_p<0\n",
    "$$\n",
    "\n",
    "Como el hiperplano divide el espacio en 2 partes, podemos utilizar esta herramienta para que aquellas observaciones con sus caracteristicas $X_p$ den un valor mayor al del hiperplano corresponderán a una clase, y las que den un valor menor corresponderán a la otra clase. \n",
    "\n",
    "## The Maximal Margin Classifier.\n",
    "De forma general, si existe un hiperplano que pueda dividir nuestros datos en 2 subespacios, entonces existirán infinitos hiperplanos que puedan dividir nuestras muestras. \n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_SVM/planos_separadores.png\" style=\"width:50%;\">\n",
    "</div>\n",
    "Como podemos ver en la gráfica de la izquierda de la imagen anterior, todos los planos mostrados son capaces de divir las muestras según su clasificación, pero no todas lo hacen de la misma manera, así que debemos elegir aquel plano que mejor lo haga, pero ¿Como lo hacemos?\n",
    "Usando el <span style=\"color: IndianRed;\">Maximal Margin Hiperplane</span> (También conocido como el <i>plano separador óptimo</i>), el cúal sería el plano que esté <u>más separado de los puntos en el espacio</u>. Para encontrar dicho plano debemos calcular la distancia perpendicular desde cada observación al hiperplano separador.\n",
    "Aquella observación que tenga la <u>distancia más pequeña</u> será aquella que defina el <b>margen</b>. Y aquel hiperplano que tenga el <u>mayor <b>margen</b></u> será el plano que eligamos. \n",
    "La observación que tenga la <u>distancia más pequeña</u> es llamada como <b>support vector</b>, ya que \"soporta\" el margen maximo del hiperplano. El hiperplano decidido depende únicamente de estos <b>support vector</b>, por lo que cualquier movimineto de estas observaciones significará un movimiento de esto planos. Es por ello que los <u>valores atípicos tienen mucho peso</u>\n",
    "\n",
    "Aunque pensemos que el **Maximal Margin Classifier** es el que mejores resultados suele dar, también puede chocar con *overfitting*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42ff7e2",
   "metadata": {},
   "source": [
    "\n",
    "## Caso No-Separable -> Support Vector Classifier\n",
    "En muchos casos, es imposible encontrar un hiperplano que sea capaz de dividir las muestras etiquetadas, o incluos puede haber planos que dividan las muestras, pero estos planos (debidos a su poca generalización) no son deseables. \n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_SVM/influencia_de_una_observacion.png\" style=\"width:50%;\">\n",
    "</div>\n",
    "Como podemos ver en el caso anterior, en la izquierda tenemos un hiperplano que separa bastante bien las dos clases. En la gráfica de la derecha, hemos añadido un punto que modifica completamente el hiperplano anterior, perdiendo generalidad al solo implementar 1 punto. \n",
    "Es por eso que tal vez pueda ser rentable disclasificar algunas observaciones que únicamente añaden ruido o error al modelo, para así conseguir mas:\n",
    "* Mayor robustez a observaciones individuales.\n",
    "* Mayor clasificación para la mayoría de las observaciones de entrenamiento.\n",
    "El <b>Suport vector classifier</b>, también llamado como <b>Soft Margin Classifier</b>, lo que hace es: en vez de buscar el mayor <i>margen</i> posible, permite que algunas observaciones estén en el lado incorrecto del margen, incluso en el lado incorrecto del subespacio -> Es por esto que se llama <b>margen suave</b> (porque no es un margen estricto). \n",
    "En la imagen siguiente se muestra un ejemplo de como serían este tipo de observaciones.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_SVM/clasificador_por_margen_suave.png\" style=\"width:50%;\">\n",
    "</div>\n",
    "\n",
    "A modo de resumen: Un <b>Support Vector Classifier</b> elige un hierplano que es capaz de separar <u>la mayoria de las observaciones de training</u>, pero tal vez <u>algunas observaciones se clasisfican incorrectamente</u>\n",
    "\n",
    "Existe un hiperparametro <b>C</b> que se puede entender como una especie de \"budget\" a la cantidad de <i>margen</i> que puede ser violado por las <i>n</i> observaciones.\n",
    "* Si usamos **C**=0, entonces estariamos clasisficando con *Maximal Margin Classifier*\n",
    "\n",
    "En la práctica **C** es un hiperparametro que se calcula mediante **Cross-validation**. Este hiperparametro controla el *bias-variance trade-off*.\n",
    "* Cuando **C** es pequeño: buscamos *margenes pequeños* -> lo que nos da **Baja bias** pero **Alta Varianza**. Tambien indicará que habrá pocos *support vectors*\n",
    "* Caundo **C** es alto: tendremso *margenes altos* -> lo que nos da **Alta bias** pero **Baja Varianza**. También habrá muchos *support vectors*\n",
    "\n",
    "| Valor de **C** | Margen          | Bias        | Varianza     | Nº de Support Vectors |\n",
    "|----------------|------------------|-------------|--------------|------------------------|\n",
    "| **Pequeño**    | *Pequeño*        | **Bajo**    | **Alto**     | *Pocos*                |\n",
    "| **Alto**       | *Grande*         | **Alto**    | **Bajo**     | *Muchos*               |\n",
    "\n",
    "En la siguiente imagen podemos ver ejemplos de distintos valores para el hiperparametro **C**\n",
    "\n",
    "|                      | **Izquierda**            | **Derecha**              |\n",
    "|----------------------|--------------------------|---------------------------|\n",
    "| **Arriba**           | 🔺 Mayor valor            | 🔺 Segundo valor más alto |\n",
    "| **Abajo**            | 🔻 Tercer valor más alto  | 🔻 Valor más bajo         |\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_SVM/diferentes_valores_C.png\" style=\"width:50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f67b6",
   "metadata": {},
   "source": [
    "# Clasificacion con Non-linear Decision Boundaries\n",
    "## Incremento de features no lineales\n",
    "Los *Support Vector Classifier* son una opción muy natural para problemas de variable dependiente dicotomiica, si la frontera entre estas variables el **lineal**. Pero siempre pueden existir fronteras no-lineales. Podemos ver un ejemplo:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes_SVM/non_lineal_boundaries.png\" style=\"width:50%;\">\n",
    "</div>\n",
    "\n",
    "Para poder resolver este problema podemos aumentar el **feature space** usando funciones cuadraticas, cubicas o mayores. Por ejemplo en vez de usar *p* variables usamos *2p* variables de la siguiente forma:\n",
    "\n",
    "$$\n",
    "X_1, X_2, ..., X_p \\quad \\longrightarrow \\quad X_1, X_1^2, X_2, X_2^2, ..., X_p, X_p^2\n",
    "$$\n",
    "\n",
    "Este proceso aumenta las dimensiones de calculo, haciendolo computacionalmente más costoso y a veces inmanegable, por eso es mas recomendable usar lo siguiente.\n",
    "\n",
    "# Uso de kernels SVM\n",
    "\n",
    "El *Support Vector Machine* es una extensión del *Support Vecto Classifier* que resulta de alargar el *feature space* usando **kernels**. \n",
    "(La explicación del uso de **kernels** no viene muy detallada en el libro de *An Introduction to statistical learning* así que intentaré hacer un resumen.)\n",
    "La idea principal es en resumen lo explicado en el apartado [Incremento de features no lineales](#Incremento-de-features-no-lineales)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
