{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1584a450",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "## Datos iniciales\n",
    "* $\\mathbf{x}$ = Entradas\n",
    "* $\\mathbf{r}$ = Reconstrucción de las entradas.\n",
    "* $\\mathbf{h}$ = Hidden layers\n",
    "\n",
    "Un **Autoencoder** es una red neuronal que ha sido entrenada para copiar sus entradas a sus salidas. Internamente, tiene una capa oculta ($\\mathbf{h}$) que describe un <u>código usado para representar su entrada</u>\n",
    "\n",
    "Tiene dos partes:\n",
    "* *Función de codificación*($f$): $\\mathbf{h}=f(x)$\n",
    "* *Función de decodificación*($g$): $\\mathbf{r}=g(\\mathbf{h})$\n",
    "\n",
    "Si nuestro autoencoder simplemente aprende a hacer esto: $g(f(\\mathbf{x}))=\\mathbf{x}$, entonces no sería útil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270e43d",
   "metadata": {},
   "source": [
    "Aunque parezca ridículo usar una red neuronal que su única función es copiar la entrada en la salida, realmente nosotros no estamos interesados en la salida del decoder, estamos interesados en en las capas ocultas del autoencoder.\n",
    "\n",
    "## Undercomplete Autoencoder.\n",
    "Una forma de encontrar *features* interesantes sería haciendo que la capa oculta $\\textit{\\textbf{h}}$ tenga menos dimensiones que $\\textit{\\textbf{x}}$. <u> De esta manera forzaremos al autoencoder a capturar las características más importantes de los datos de entrenamiento.</u>\n",
    "\n",
    "El proceso de aprendizaje será simplemente minimizar la función de perdida:\n",
    "$$\n",
    "L(\\textit{\\textbf{x}, g(f(\\textit{\\textbf{x}}))})\n",
    "$$\n",
    "\n",
    "* En el caso de que el decoder sea lineal y $L=MSA$ -> Estaremos aprendiendo el mismo subespacio que con un PCA.\n",
    "Los autoencoders con funciones de codificación no lineales y funciones de decodificación no lineales podrán aprender una generalización no lineal, que es más potente que las PCA.\n",
    "\n",
    "## Regularized Autoencoder.\n",
    "Los **Regularized Autoencoders** usa una función de pérdida que anima al modelo para que tenga otras propiedades a parte de copiar el input al output.\n",
    "\n",
    "### Sparse Autoencoders\n",
    "Un **Sparse Autoencoder** es simplemente un autoencoder, el cual tiene una **sparsity penalty** $\\Omega(\\textit{\\textbf{h}})$ en las capas $\\textit{\\textbf{h}}$:\n",
    "$$\n",
    "L(\\textit{\\textbf{x}}, g(f(\\textit{\\textbf{x}})))+\\Omega(\\textit{\\textbf{h}})\n",
    "$$\n",
    "\n",
    "Este tipo de autoencoder regularizado debe responder a características estadísticas únicas del dataset que ha sido entrenado\n",
    "\n",
    "# Poder de representación, tamaño y profundidad de las capas.\n",
    "Usualmente son entrenadas con una capa oculta para el encoder y otra para el decoder, pero esto no es un requerimiento. De hecho usar decoders y encoders profundos puede tener muchas ventajas.\n",
    "\n",
    "Ventajas:\n",
    "* El *teorema del aproximador lineal* garantiza que una feedforward red neuronal con al menos una capa oculta puede representar una aproximación de cualquier función.\n",
    "\n",
    "Experimentalmente, los autoencoders profundos nos tienen mucha mejor comprensión que sus correspondientes autoencoders lineales o superficiales.\n",
    "\n",
    "Una estrategia común es entrenar autoencoder superficiales, y en el momento que encontremos una buena, usaremos un autoencoder profundo.\n",
    "\n",
    "\n",
    "# Denoising Autoencoders.\n",
    "Los DAE *Autoencoders quitarruido* son autoencoders que reciben como entrada una base de datos corrupta y deben predecir en su salida la base de datos sin estar corrompida.\n",
    "\n",
    "Introducimos el *proceso de corrupción*, el cuál representa una distribución condicional de las muestras corruptas $\\textit{\\textbf{\\widetilde{x}}}$, teniendo las muestras $\\textit{\\textbf{x}}$. El autoencoder entonces aprenderá la **distribución de reconstrucción** $\\textit{p_{reconstruct}}(\\textit{\\textbf{x}}|\\textit{\\textbf{\\widetilde{x}}})$\n",
    "\n",
    "El proceso es el siguiente:\n",
    "1. Tenemos nuestros datos de entrenamiento: $\\textit{\\textbf{x}}$\n",
    "2. Corrompemos estos datos $\\textit{\\textbf{\\widetilde{x}}}$\n",
    "3. Usamos ($\\textit{\\textbf{x}}$, $\\textit{\\textbf{\\widetilde{x}}}$) como muestras totales de entrenamiento para generar nuestro modelo\n",
    "\n",
    "\n",
    "# Manifolds con Autoencoders.\n",
    "Como muchos otros algoritmos de  machine learning, los autoencoders explotan la idea de que los datos se concentran alrededor de un *mainfold* de menor dimensión.\n",
    "\n",
    "Para entender cómo explotan los autoencoder los *manifolds* debemos tener en cuenta una caracterización importante de los *manifolds*.\n",
    "Una *caracterización importante* de los *manifolds* es el **set de planos tangentes**.\n",
    "En un punto $\\textit{x}$ de un *manifold* d-dimensional, el plano tangente es dado por *d* vectores básicos "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
