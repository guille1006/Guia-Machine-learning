{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a6523f",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 3em; font-weight: bold; text-align: center;\">Transformers Anatomy</div>\n",
    "<div style=\"text-align: right; font-size: 24px; margin-right: 10px;\">Guillermo Díaz Aguado</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598b8d4",
   "metadata": {},
   "source": [
    "# 1 La Arquitectura de los transformadores\n",
    "Los transformadores originalmente se basaban en la estructura de los *encoder-decoder*. \n",
    "\n",
    "**Encoder**:\n",
    "* Convierte una **secuencia de tokens de entrada** en una **secuencia de vectores embebidos**, estos últimos suelen ser llamados como **hidden state** o **contexto**.\n",
    "\n",
    "**Decoder**\n",
    "* Usa el **hiden state** del **encoder** para generar iterativamente una **secuencia de tokens de salida**.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes/Arquitectura encoder-decoder.png\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "Aunque existan cientos de arquitecturas distintas para los transformadores, muchas de ellas pertenecen a una de las siguientes clases:\n",
    "\n",
    "*Encoder-only*:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Estos modelos convierten una secuencia de entrada en una representación numerica. Donde más se ve es en *text classification*.\n",
    "\n",
    "*Decoder-only*:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Suelen ser usados para autocompletar texto mediante la predicción de la palabra más probable.\n",
    "\n",
    "*Encoder-Decoder*:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Encajan en tareas de traducción \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbb8a6",
   "metadata": {},
   "source": [
    "## 1. 2 El Encoder\n",
    "El **encoder** consiste en muchas capas apiladas una al lado de otra. Simplemente cada capa recide una *secuencia de tokens embebidos* y se las pasa a la siguiente capa mediante las siguientes *sub-capas*:\n",
    "* Una capa **multi-head self-attention**: sirve para que preste **atención** a diferentes partes de la secuencia al mismo tiempo y de formas distintas.\n",
    "    * *Self attention*: implica que cada elemento se relacione a los demás de una manera.\n",
    "    * *Multi-head*: genera varias relaciones en paralelo para que aprenda de forma más rica y variada. \n",
    "* Una capa **fully connected feed forward** que se aplica a cada entrada.  \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"Imagenes/encoder.png\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "<div style=\"border: 10px solid black; padding: 10px; width:100%\">\n",
    "    El rol principal del Encoder es actualizar las entradas embebidas para producir representaciones con más información contextual.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef9aee",
   "metadata": {},
   "source": [
    "### 1.2.1 Self-Attention\n",
    "Es un mecanismo que permite a las redes neuronales <u> asignar diferentes pesos o \"attention\"</u> a cada elemento en la secuencia. \n",
    "Lo que se hace es dada una secuencia de *token embebidos* $[x_1, \\ldots, x_n]$, el proceso de **self-attention** produce una nueva secuencia de *tokens embebidos* $[x'_1, \\ldots, x'_n]$ donde cada $x'_i$ es una combinación lineal de todos los tokens $x_j$. Estos embeddings se llaman **contextualized embeddings**.\n",
    "$$\n",
    "x'_i = \\sum^n_{j=1}w_{ji}x_j\n",
    "$$\n",
    "\n",
    "La idea principal de este proceso es que en vez de tener unos tokens de entrada que siempre vayan a tener los mismos valores, tengamos unos tokens que cambien su valor en función del contexto. Si hablamos de \"apple\" en un entorno financiero, se referirá a la compañía y no a la fruta.\n",
    "\n",
    "Los coeficientes $w_{ji}$ son llamados **attention weights** y <u>están normalizados</u>, lo que significa que $\\sum w{ji} =1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e9615",
   "metadata": {},
   "source": [
    "#### Formas de implementar Self-Attention\n",
    "##### Scaled dot-product attention\n",
    "1. Proyectamos cada uno de los tokens embebidos en 3 vectores llamados *query*, *key* y *value*.\n",
    "2. Computación de los **attention scores**. Determinamos como se relacionan los vectores *query* y *key* usando una **similarity function**. Para la **Scaled dot-product attention** usaremos como función de similaridad el *dot product* de las matrices. Aquellas *query* y *keys* que sean similiares tendrán un valor alto, y las que no sean similares tendrán un valor bajo. El valor que nos da esta función se llama **attention scores**.\n",
    "3. Computación de los **pesos/weights**. La función *dot-product* puede devolver valores muy dispares y muy altos para arreglar esto se realiza:\n",
    "    * *attention scores* son multiplicados por un factor de escalado para <u>normalizar la varianza</u>\n",
    "    * Después se normaliza con una *softmax* para que la suma de los pesos sea 1. \n",
    "4. Actualizamos los tokens embebidos. Una vez tenemos los *pesos* y con el vector *values*, usamos estos valores para sacar los **contextualized tokens** de la siguiente forma:\n",
    "$$\n",
    "x'_i = \\sum^n_{j=1}w_{ji}v_j\n",
    "$$\n",
    "\n",
    "Como podemos entender los vectores **query**, **key** y **value** usando la siguiente analogía:\n",
    "Imagina que estas en el supermercado \n",
    "    * Tienes una lista con todos los ingredientes de tu cena -> *query*\n",
    "    * Las etiquetas del supermercado -> *keys*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e449d7c",
   "metadata": {},
   "source": [
    "### 1.2.2 Multi-headed attention\n",
    "Como hemso explicado antes, para el proceso de *self-attention* realizamos 3 transformaciones linealmente independientes para generar los vectores de *key*, *query* y *value*. Estas transformaciones permiten a la capa de *attention* que se enfoque en un aspecto semántico.\n",
    "\n",
    "Pero sería beneficioso realizar estas transformaciones con otras proyecciones lineales para sacar distintos aspectos semánticos, cada una de estas proyecciones se llama **attention head**. Al realizar muchas *attention head* tendremos como resultado una **multihead attention layer**.\n",
    "\n",
    "Así cada \"cabeza\" se puede enfocar en algo: tal vez una se fije en la interacción sustantivo-verbo, otra se centre en los adjetivos..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
